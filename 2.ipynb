{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.makedirs('/tmp/project', exist_ok=True)\n",
        "os.chdir('/tmp/project')\n",
        "print( os.getcwd() )\n",
        "if not os.path.exists('/tmp/project/train.csv'):\n",
        "    !cp /content/drive/MyDrive/Colab_Notebooks/dacon/2024_저해상도조류이미지/open.zip /tmp/project\n",
        "    !unzip -o -q open.zip\n",
        "    !rm open.zip\n",
        "    # 추가 모듈 설치\n",
        "    !sudo apt-get install -y libmagickwand-dev\n",
        "    !pip install wandb timm wand"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZAN0JGuIf7z",
        "outputId": "ac347a22-c542-428a-89c7-0903aaf93bba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/tmp/project\n",
            "cp: cannot stat '/content/drive/MyDrive/Colab_Notebooks/dacon/2024_저해상도조류이미지/open.zip': No such file or directory\n",
            "unzip:  cannot find or open open.zip, open.zip.zip or open.zip.ZIP.\n",
            "rm: cannot remove 'open.zip': No such file or directory\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 ghostscript\n",
            "  gir1.2-freedesktop gir1.2-gdkpixbuf-2.0 gir1.2-rsvg-2.0 gsfonts\n",
            "  imagemagick-6-common libblkid-dev libblkid1 libcairo-script-interpreter2\n",
            "  libcairo2-dev libdjvulibre-dev libdjvulibre-text libdjvulibre21 libffi-dev\n",
            "  libfftw3-double3 libgdk-pixbuf-2.0-dev libgdk-pixbuf2.0-bin libglib2.0-dev\n",
            "  libglib2.0-dev-bin libgs9 libgs9-common libice-dev libidn12 libijs-0.35\n",
            "  libjbig2dec0 libjxr-tools libjxr0 liblcms2-dev liblqr-1-0 liblqr-1-0-dev\n",
            "  liblzo2-2 libmagickcore-6-arch-config libmagickcore-6-headers\n",
            "  libmagickcore-6.q16-6 libmagickcore-6.q16-6-extra libmagickcore-6.q16-dev\n",
            "  libmagickwand-6-headers libmagickwand-6.q16-6 libmagickwand-6.q16-dev\n",
            "  libmount-dev libmount1 libpixman-1-dev librsvg2-common librsvg2-dev\n",
            "  libselinux1-dev libsepol-dev libsm-dev libwmf-0.2-7 libwmf-dev\n",
            "  libwmflite-0.2-7 libxcb-render0-dev libxcb-shm0-dev libxt-dev poppler-data\n",
            "Suggested packages:\n",
            "  fonts-noto fonts-freefont-otf | fonts-freefont-ttf fonts-texgyre\n",
            "  ghostscript-x libcairo2-doc libfftw3-bin libfftw3-dev libgirepository1.0-dev\n",
            "  libglib2.0-doc libxml2-utils libice-doc inkscape cryptsetup-bin librsvg2-doc\n",
            "  libsm-doc libwmf-0.2-7-gtk libwmf-doc libxt-doc poppler-utils\n",
            "  fonts-japanese-mincho | fonts-ipafont-mincho fonts-japanese-gothic\n",
            "  | fonts-ipafont-gothic fonts-arphic-ukai fonts-arphic-uming fonts-nanum\n",
            "The following NEW packages will be installed:\n",
            "  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 ghostscript\n",
            "  gir1.2-freedesktop gir1.2-gdkpixbuf-2.0 gir1.2-rsvg-2.0 gsfonts\n",
            "  imagemagick-6-common libblkid-dev libcairo-script-interpreter2 libcairo2-dev\n",
            "  libdjvulibre-dev libdjvulibre-text libdjvulibre21 libffi-dev\n",
            "  libfftw3-double3 libgdk-pixbuf-2.0-dev libgdk-pixbuf2.0-bin libglib2.0-dev\n",
            "  libglib2.0-dev-bin libgs9 libgs9-common libice-dev libidn12 libijs-0.35\n",
            "  libjbig2dec0 libjxr-tools libjxr0 liblcms2-dev liblqr-1-0 liblqr-1-0-dev\n",
            "  liblzo2-2 libmagickcore-6-arch-config libmagickcore-6-headers\n",
            "  libmagickcore-6.q16-6 libmagickcore-6.q16-6-extra libmagickcore-6.q16-dev\n",
            "  libmagickwand-6-headers libmagickwand-6.q16-6 libmagickwand-6.q16-dev\n",
            "  libmagickwand-dev libmount-dev libpixman-1-dev librsvg2-common librsvg2-dev\n",
            "  libselinux1-dev libsepol-dev libsm-dev libwmf-0.2-7 libwmf-dev\n",
            "  libwmflite-0.2-7 libxcb-render0-dev libxcb-shm0-dev libxt-dev poppler-data\n",
            "The following packages will be upgraded:\n",
            "  libblkid1 libmount1\n",
            "2 upgraded, 56 newly installed, 0 to remove and 47 not upgraded.\n",
            "Need to get 34.8 MB of archives.\n",
            "After this operation, 123 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 imagemagick-6-common all 8:6.9.11.60+dfsg-1.3ubuntu0.22.04.5 [64.3 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libmagickcore-6-headers all 8:6.9.11.60+dfsg-1.3ubuntu0.22.04.5 [52.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libmagickcore-6-arch-config amd64 8:6.9.11.60+dfsg-1.3ubuntu0.22.04.5 [26.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfftw3-double3 amd64 3.3.8-2ubuntu8 [770 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 liblqr-1-0 amd64 0.4.2-2.1 [27.7 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libmagickcore-6.q16-6 amd64 8:6.9.11.60+dfsg-1.3ubuntu0.22.04.5 [1,795 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdjvulibre-text all 3.5.28-2build2 [50.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdjvulibre21 amd64 3.5.28-2build2 [624 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libmagickwand-6.q16-6 amd64 8:6.9.11.60+dfsg-1.3ubuntu0.22.04.5 [328 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwmflite-0.2-7 amd64 0.2.12-5ubuntu1 [68.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libmagickcore-6.q16-6-extra amd64 8:6.9.11.60+dfsg-1.3ubuntu0.22.04.5 [70.1 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdjvulibre-dev amd64 3.5.28-2build2 [2,463 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblcms2-dev amd64 2.12~rc1-2build2 [1,887 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/universe amd64 liblqr-1-0-dev amd64 0.4.2-2.1 [69.1 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 gir1.2-freedesktop amd64 1.72.0-1 [22.3 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gir1.2-gdkpixbuf-2.0 amd64 2.42.8+dfsg-1ubuntu0.3 [9,486 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gir1.2-rsvg-2.0 amd64 2.52.5+dfsg-3ubuntu0.2 [16.5 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcairo-script-interpreter2 amd64 1.16.0-5ubuntu2 [62.0 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice-dev amd64 2:1.0.10-1build2 [51.4 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm-dev amd64 2:1.2.3-1build2 [18.1 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpixman-1-dev amd64 0.40.0-1ubuntu0.22.04.1 [280 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render0-dev amd64 1.14-3ubuntu3 [19.6 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-shm0-dev amd64 1.14-3ubuntu3 [6,848 B]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 libffi-dev amd64 3.4.2-4 [63.7 kB]\n",
            "Ign:27 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libglib2.0-dev-bin amd64 2.72.4-0ubuntu2.3\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libblkid1 amd64 2.37.2-4ubuntu3.4 [103 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libblkid-dev amd64 2.37.2-4ubuntu3.4 [185 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libmount1 amd64 2.37.2-4ubuntu3.4 [122 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsepol-dev amd64 3.3-1build1 [378 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 libselinux1-dev amd64 3.3-1build2 [158 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libmount-dev amd64 2.37.2-4ubuntu3.4 [14.5 kB]\n",
            "Ign:34 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libglib2.0-dev amd64 2.72.4-0ubuntu2.3\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcairo2-dev amd64 1.16.0-5ubuntu2 [692 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgdk-pixbuf2.0-bin amd64 2.42.8+dfsg-1ubuntu0.3 [14.2 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgdk-pixbuf-2.0-dev amd64 2.42.8+dfsg-1ubuntu0.3 [47.8 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-dev amd64 2.52.5+dfsg-3ubuntu0.2 [49.8 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwmf-0.2-7 amd64 0.2.12-5ubuntu1 [94.2 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwmf-dev amd64 0.2.12-5ubuntu1 [236 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Err:27 http://security.ubuntu.com/ubuntu jammy-updates/main amd64 libglib2.0-dev-bin amd64 2.72.4-0ubuntu2.3\n",
            "  404  Not Found [IP: 185.125.190.83 80]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libmagickcore-6.q16-dev amd64 8:6.9.11.60+dfsg-1.3ubuntu0.22.04.5 [1,120 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libmagickwand-6-headers all 8:6.9.11.60+dfsg-1.3ubuntu0.22.04.5 [10.4 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libmagickwand-6.q16-dev amd64 8:6.9.11.60+dfsg-1.3ubuntu0.22.04.5 [358 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libmagickwand-dev all 8:6.9.11.60+dfsg-1.3ubuntu0.22.04.5 [1,178 B]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n",
            "Err:34 http://security.ubuntu.com/ubuntu jammy-updates/main amd64 libglib2.0-dev amd64 2.72.4-0ubuntu2.3\n",
            "  404  Not Found [IP: 185.125.190.83 80]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.10 [752 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.10 [5,031 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ghostscript amd64 9.55.0~dfsg1-0ubuntu5.10 [49.4 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/universe amd64 gsfonts all 1:8.11+urwcyr1.0.7~pre44-4.5 [3,120 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjxr0 amd64 1.2~git20170615.f752187-5 [174 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjxr-tools amd64 1.2~git20170615.f752187-5 [16.0 kB]\n",
            "Fetched 32.9 MB in 4s (9,317 kB/s)\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/g/glib2.0/libglib2.0-dev-bin_2.72.4-0ubuntu2.3_amd64.deb  404  Not Found [IP: 185.125.190.83 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/g/glib2.0/libglib2.0-dev_2.72.4-0ubuntu2.3_amd64.deb  404  Not Found [IP: 185.125.190.83 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.11)\n",
            "Collecting wand\n",
            "  Downloading Wand-0.6.13-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu121)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.26.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.6)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.0.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Downloading Wand-0.6.13-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wand\n",
            "Successfully installed wand-0.6.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "_OKnLCh0HNQV",
        "outputId": "86995694-e5cf-486e-ba66-ccfdaadfffd9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 343) (<ipython-input-2-2216c75b6137>, line 343)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-2216c75b6137>\"\u001b[0;36m, line \u001b[0;32m343\u001b[0m\n\u001b[0;31m    selective_replace('/content', '/content/drive/MyDrive/Colab Notebooks/open\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 343)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import gc\n",
        "import math\n",
        "import pickle\n",
        "import re\n",
        "import sys\n",
        "import logging\n",
        "import IPython\n",
        "from glob import glob\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from wand import image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, _LRScheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import swa_utils\n",
        "\n",
        "import torchvision\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import timm\n",
        "from transformers import AutoModel\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import wandb\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Configure training settings\n",
        "CFG = {\n",
        "    'SEED': 42,\n",
        "    'N_SPLIT': 5,\n",
        "    'LABEL_SMOOTHING': 0.05,\n",
        "    'OPTIMIZER': 'AdamW',\n",
        "    'INTERPOLATION': 'robidouxsharp',\n",
        "    'PRECISION': '16',\n",
        "    'MODEL_NAME': \"timm/deit3_large_patch16_224.fb_in22k_ft_in1k\",\n",
        "    'IMG_SIZE': 224,\n",
        "    'BATCH_SIZE': 48,\n",
        "    'LR': [0.25e-5 * np.sqrt(48), 1e-7],\n",
        "    'IMG_TRAIN_SIZE': 224,\n",
        "}\n",
        "\n",
        "# Logger setup\n",
        "logger = logging.getLogger()\n",
        "logging.basicConfig(handlers=[\n",
        "    logging.StreamHandler(stream=sys.stdout),\n",
        "    logging.handlers.RotatingFileHandler(filename='run.log', mode='a', maxBytes=512000, backupCount=4)\n",
        "])\n",
        "logging_formatter = logging.Formatter('%(asctime)s [%(levelname)-4.4s] %(message)s', datefmt='%m/%d %H:%M:%S')\n",
        "_ = [h.setFormatter(logging_formatter) for h in logger.handlers]\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "def showtraceback(self, *args, **kwargs):\n",
        "    logger.exception('-------Exception----------')\n",
        "\n",
        "IPython.core.interactiveshell.InteractiveShell.showtraceback = showtraceback\n",
        "logger.info('program started')\n",
        "\n",
        "# Seed function\n",
        "def seed_everything(seed):\n",
        "    logger.info(f'seed_everything : {seed}')\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(CFG['SEED'])\n",
        "\n",
        "# Function to selectively replace files\n",
        "def selective_replace(src_folder, dest_folder, replace_ext=None, skip_ext='.jpg'):\n",
        "    for root, _, files in os.walk(src_folder):\n",
        "        relative_path = os.path.relpath(root, src_folder)\n",
        "        dest_root = os.path.join(dest_folder, relative_path)\n",
        "\n",
        "        if not os.path.exists(dest_root):\n",
        "            os.makedirs(dest_root)\n",
        "\n",
        "        for file in files:\n",
        "            src_file = os.path.join(root, file)\n",
        "            dest_file = os.path.join(dest_root, file)\n",
        "\n",
        "            if skip_ext and file.endswith(skip_ext):\n",
        "                print(f\"Skipping: {file}\")\n",
        "                continue\n",
        "\n",
        "            if os.path.exists(dest_file):\n",
        "                if skip_ext and file.endswith(skip_ext):\n",
        "                    print(f\"Skipping: {file}\")\n",
        "                    continue\n",
        "                else:\n",
        "                    shutil.copy2(src_file, dest_file)\n",
        "                    print(f\"Replaced: {file}\")\n",
        "\n",
        "# Custom Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, img_path_list, label_list, load_img_size, shuffle=False, transforms=None, interpolation='robidouxsharp'):\n",
        "        self.df = pd.DataFrame({'img_path_list': img_path_list})\n",
        "        self.interpolation = interpolation\n",
        "        self.load_img_size = load_img_size\n",
        "        logger.info(f'load_img_size={load_img_size}')\n",
        "        if label_list is not None:\n",
        "            self.df['label_list'] = label_list\n",
        "        if shuffle:\n",
        "            self.df = self.df.sample(frac=1.0).reset_index(drop=True)\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def get_interpolated_image(self, img, new_image_size):\n",
        "        if self.interpolation == 'pil_lanczos':\n",
        "            if isinstance(img, np.ndarray):\n",
        "                img = Image.fromarray(img)\n",
        "            return img.resize((new_image_size, new_image_size), Image.LANCZOS)\n",
        "        elif self.interpolation == 'cv2_lanczos4':\n",
        "            if not isinstance(img, np.ndarray):\n",
        "                img = np.array(img)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "            img = cv2.resize(img, (new_image_size, new_image_size), interpolation=cv2.INTER_LANCZOS4)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            return Image.fromarray(img)\n",
        "        else:\n",
        "            if not isinstance(img, np.ndarray):\n",
        "                img = np.array(img)\n",
        "            with image.Image.from_array(img) as src:\n",
        "                src.resize(new_image_size, new_image_size, filter=self.interpolation)\n",
        "                return Image.fromarray(np.array(src))\n",
        "\n",
        "    def get_image_from_index(self, index, img_size):\n",
        "        img_path = self.df.img_path_list[index]\n",
        "        fname = img_path.replace('./','').split('.')[0] + '.png'\n",
        "        full_fname = f'img_cached/{img_size}_{self.interpolation}/{fname}'\n",
        "        if os.path.exists(full_fname):\n",
        "            img = Image.open(full_fname)\n",
        "        else:\n",
        "            fname_path = '/'.join(full_fname.split('/')[:-1])\n",
        "            os.makedirs(fname_path, exist_ok=True)\n",
        "            img = self.get_interpolated_image(Image.open(img_path), img_size)\n",
        "            img.save(full_fname)\n",
        "        return img\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.get_image_from_index(index, self.load_img_size)\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image=np.array(image))['image']\n",
        "        if 'label_list' in self.df.columns:\n",
        "            label = self.df.label_list[index]\n",
        "            return {'pixel_values': image, 'label': label}\n",
        "        else:\n",
        "            return {'pixel_values': image}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "# CosineAnnealingWarmupRestarts Scheduler\n",
        "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
        "    def __init__(self,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 first_cycle_steps: int,\n",
        "                 cycle_mult: float = 1.,\n",
        "                 max_lr: float = 1e-5,\n",
        "                 min_lr: float = 1e-10,\n",
        "                 warmup_steps: int = 0,\n",
        "                 gamma: float = 1.,\n",
        "                 last_epoch: int = -1):\n",
        "        assert warmup_steps < first_cycle_steps\n",
        "        self.first_cycle_steps = first_cycle_steps\n",
        "        self.cycle_mult = cycle_mult\n",
        "        self.base_max_lr = max_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.min_lr = min_lr\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.gamma = gamma\n",
        "        self.cur_cycle_steps = first_cycle_steps\n",
        "        self.cycle = 0\n",
        "        self.step_in_cycle = last_epoch\n",
        "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
        "        self.init_lr()\n",
        "\n",
        "    def init_lr(self):\n",
        "        self.base_lrs = [self.min_lr for _ in self.optimizer.param_groups]\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = self.min_lr\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.step_in_cycle == -1:\n",
        "            return self.base_lrs\n",
        "        elif self.step_in_cycle < self.warmup_steps:\n",
        "            return [(self.max_lr - base_lr) * self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            return [base_lr + (self.max_lr - base_lr) *\n",
        "                    (1 + math.cos(math.pi * (self.step_in_cycle - self.warmup_steps) /\n",
        "                                 (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
        "                    for base_lr in self.base_lrs]\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "            self.step_in_cycle += 1\n",
        "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
        "                self.cycle += 1\n",
        "                self.step_in_cycle -= self.cur_cycle_steps\n",
        "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
        "        else:\n",
        "            if epoch >= self.first_cycle_steps:\n",
        "                if self.cycle_mult == 1.:\n",
        "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
        "                    self.cycle = epoch // self.first_cycle_steps\n",
        "                else:\n",
        "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
        "                    self.cycle = n\n",
        "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
        "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** n\n",
        "            else:\n",
        "                self.cur_cycle_steps = self.first_cycle_steps\n",
        "                self.step_in_cycle = epoch\n",
        "        self.max_lr = self.base_max_lr * (self.gamma ** self.cycle)\n",
        "        self.last_epoch = math.floor(epoch)\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "# Custom Model with GroupNorm and Dropout\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, model, dropout_rate=0.5):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.model = model\n",
        "        hidden_dim = model.num_features if hasattr(model, 'num_features') else model.config.hidden_size\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.clf = nn.Sequential(\n",
        "            nn.GroupNorm(num_groups=32, num_channels=hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(hidden_dim, 25)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        if not isinstance(x, torch.Tensor):\n",
        "            x = x.pooler_output\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "\n",
        "# Test-Time Augmentation\n",
        "tta_transforms = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.Rotate(limit=15, p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "def prediction_with_tta(model, test_loader, device, n_augmentations=5):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"TTA Prediction\"):\n",
        "            batch_preds = []\n",
        "            for _ in range(n_augmentations):\n",
        "                augmented = tta_transforms(image=batch['pixel_values'].cpu().numpy())\n",
        "                augmented_image = augmented['image'].to(device)\n",
        "                output = model(augmented_image)\n",
        "                batch_preds.append(F.softmax(output, dim=1).cpu().numpy())\n",
        "            batch_preds = np.mean(batch_preds, axis=0)\n",
        "            preds.append(batch_preds)\n",
        "    return np.concatenate(preds, axis=0)\n",
        "\n",
        "# Early Stopping\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=False, delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, score):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "\n",
        "# Create Model Function\n",
        "def create_model(model_name, freeze_layers=True):\n",
        "    logger.info(f'create_model: {model_name}')\n",
        "    if '/' not in model_name:\n",
        "        model_name = 'timm/' + model_name\n",
        "    if model_name.startswith('timm/'):\n",
        "        base_model = timm.create_model(model_name, pretrained=True)\n",
        "    else:\n",
        "        base_model = AutoModel.from_pretrained(model_name)\n",
        "    model = CustomModel(base_model)\n",
        "    if freeze_layers:\n",
        "        for param in model.model.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in model.clf.parameters():\n",
        "            param.requires_grad = True\n",
        "    model.eval()\n",
        "    model(torch.rand((1,3,CFG['IMG_SIZE'],CFG['IMG_SIZE'])).type(torch.float32))\n",
        "    return model\n",
        "\n",
        "# Load Data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content\n",
        "\n",
        "!unzip -qq \"/content/drive/MyDrive/Colab Notebooks/open.zip\"\n",
        "\n",
        "current_path = os.getcwd()\n",
        "print(current_path)\n",
        "\n",
        "# Add selective_replace function call here to selectively replace files in the dataset\n",
        "selective_replace('/content', '/content/drive/MyDrive/Colab Notebooks/open\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/train.csv')\n",
        "le = LabelEncoder()\n",
        "train_df['class'] = le.fit_transform(train_df['label'])\n",
        "\n",
        "# Save Preprocessed Data\n",
        "with open('train_subset.pkl', 'wb') as f:\n",
        "    pickle.dump(train_subset_df, f)\n",
        "\n",
        "with open('val_fold.pkl', 'wb') as f:\n",
        "    pickle.dump(val_fold_df, f)\n",
        "\n",
        "# Create Sample Dataset\n",
        "train_subset_df, _ = train_test_split(\n",
        "    train_df,\n",
        "    stratify=train_df['label'],\n",
        "    test_size=0.9,\n",
        "    random_state=CFG['SEED']\n",
        ")\n",
        "\n",
        "if not len(train_df) == len(os.listdir('/content/train')):\n",
        "    raise ValueError()\n",
        "\n",
        "skf = StratifiedKFold(n_splits=CFG['N_SPLIT'], random_state=CFG['SEED'], shuffle=True)\n",
        "\n",
        "image_size = CFG['IMG_SIZE']\n",
        "\n",
        "# Advanced Data Augmentation with MixUp and CutMix\n",
        "def get_mixup_cutmix_augmentation(alpha=1.0, prob=0.5):\n",
        "    return A.OneOf([\n",
        "        A.CutMix(p=prob, alpha=alpha),\n",
        "        A.MixUp(p=prob, alpha=alpha)\n",
        "    ], p=prob)\n",
        "\n",
        "train_transform = A.Compose([\n",
        "    A.RandomResizedCrop(image_size, image_size),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.ShiftScaleRotate(p=0.5),\n",
        "    A.OneOf([\n",
        "        A.GaussNoise(var_limit=(10.0, 50.0)),\n",
        "        A.GaussianBlur(blur_limit=(3, 7)),\n",
        "        A.MotionBlur(blur_limit=3)\n",
        "    ], p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.5),\n",
        "    A.RandomGamma(p=0.5),\n",
        "    A.HueSaturationValue(p=0.5),\n",
        "    get_mixup_cutmix_augmentation(alpha=1.0, prob=0.5),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "test_transform = A.Compose([\n",
        "    A.Resize(image_size, image_size),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset_small = CustomDataset(\n",
        "    img_path_list=train_subset_df['img_path'].values,\n",
        "    label_list=train_subset_df['label'].values,\n",
        "    load_img_size=CFG['IMG_TRAIN_SIZE'],\n",
        "    shuffle=True,\n",
        "    transforms=train_transform\n",
        ")\n",
        "\n",
        "train_loader_small = DataLoader(\n",
        "    train_dataset_small,\n",
        "    batch_size=CFG['BATCH_SIZE'],\n",
        "    shuffle=True,\n",
        "    num_workers=8,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_fold_df = None  # Initialize to avoid reference before assignment\n",
        "\n",
        "with open('val_fold.pkl', 'rb') as f:\n",
        "    val_fold_df = pickle.load(f)\n",
        "\n",
        "val_dataset = CustomDataset(\n",
        "    img_path_list=val_fold_df['img_path'].values,\n",
        "    label_list=val_fold_df['class'].values,\n",
        "    load_img_size=CFG['IMG_SIZE'],\n",
        "    shuffle=False,\n",
        "    transforms=test_transform\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CFG['BATCH_SIZE']*2,\n",
        "    shuffle=False,\n",
        "    num_workers=8,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Compute Class Weights\n",
        "class_weight = torch.FloatTensor(\n",
        "    compute_class_weight('balanced', classes=np.unique(train_df.label), y=train_df.label)\n",
        ").to('cuda')\n",
        "\n",
        "# Training Function\n",
        "def train(model, optimizer, train_loader, val_loader, scheduler, device, validation_steps=10, logging_steps=10, use_amp=True, filename=''):\n",
        "    logger.info(f'use_amp={use_amp}')\n",
        "\n",
        "    model.to(device)\n",
        "    best_score = 0\n",
        "    best_loss = 1000\n",
        "    best_model = None\n",
        "    MAX_PATIENCE = 5\n",
        "    best_patience = MAX_PATIENCE\n",
        "    loss_fn = nn.CrossEntropyLoss(weight=class_weight, label_smoothing=CFG['LABEL_SMOOTHING'], reduction='mean').to(device)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "    checkpoint_filenames = []\n",
        "    early_stopping = EarlyStopping(patience=MAX_PATIENCE, verbose=True)\n",
        "\n",
        "    max_steps = len(train_loader)\n",
        "    if not isinstance(validation_steps, int):\n",
        "        validation_steps = int(max_steps * validation_steps)\n",
        "    max_steps = (max_steps // validation_steps) * validation_steps\n",
        "\n",
        "    ema_model = swa_utils.AveragedModel(model, swa_utils.get_ema_multi_avg_fn(np.power(np.e, np.log(0.5)/(validation_steps*MAX_PATIENCE))))\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_f1': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, 4):\n",
        "        model.train()\n",
        "        train_loss = []\n",
        "        pbar_postfix = {}\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
        "        for i, batch in enumerate(pbar):\n",
        "            if i >= max_steps:\n",
        "                continue\n",
        "            steps = i + 1\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            if use_amp:\n",
        "                with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n",
        "                    output = model(batch['pixel_values'].to(device))\n",
        "                    loss = loss_fn(output, batch['label'].to(device))\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                output = model(batch['pixel_values'].to(device))\n",
        "                loss = loss_fn(output, batch['label'].to(device))\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
        "                optimizer.step()\n",
        "\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "            train_loss.append(loss.item())\n",
        "            loss = None\n",
        "            output = None\n",
        "            batch = None\n",
        "\n",
        "            if ema_model is not None:\n",
        "                ema_model.update_parameters(model)\n",
        "\n",
        "            if steps % logging_steps == 0:\n",
        "                current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "                pbar_postfix.update({\n",
        "                    't_loss0': train_loss[-1],\n",
        "                    'lr': current_lr\n",
        "                })\n",
        "                pbar.set_postfix(pbar_postfix)\n",
        "                wandb.log({\n",
        "                    \"epoch\": epoch,\n",
        "                    \"step\": steps,\n",
        "                    \"train_loss\": train_loss[-1],\n",
        "                    \"lr\": current_lr\n",
        "                }, step=(epoch-1)*max_steps + steps)\n",
        "\n",
        "            if steps % validation_steps == 0:\n",
        "                _val_loss, _val_score = validation(model, loss_fn, val_loader, device, use_amp)\n",
        "                _train_loss = np.mean(train_loss)\n",
        "                history['train_loss'].append(_train_loss)\n",
        "                history['val_loss'].append(_val_loss)\n",
        "                history['val_f1'].append(_val_score)\n",
        "\n",
        "                logger.info(f'eps={epoch}, lr={optimizer.param_groups[0][\"lr\"]:.3g}, t_loss={_train_loss:.4f}, v_loss={_val_loss:.4f}, v_f1={_val_score:.4f}')\n",
        "                wandb.log({\n",
        "                    \"epoch\": epoch,\n",
        "                    \"step\": steps,\n",
        "                    \"train_avg_loss\": _train_loss,\n",
        "                    \"valid_loss\": _val_loss,\n",
        "                    \"valid_f1\": _val_score,\n",
        "                    \"lr\": optimizer.param_groups[0][\"lr\"]\n",
        "                }, step=(epoch-1)*max_steps + steps)\n",
        "\n",
        "                early_stopping(_val_score)\n",
        "                if early_stopping.early_stop:\n",
        "                    logger.info(\"Early stopping triggered\")\n",
        "                    if ema_model is not None:\n",
        "                        swa_utils.update_bn(train_loader, ema_model, device)\n",
        "                        ema_val_loss, ema_val_score = validation(ema_model, loss_fn, val_loader, device, use_amp)\n",
        "                        logger.info(f'EMA ::: ema_v_loss={ema_val_loss:.4f}, ema_v_f1={ema_val_score:.4f}')\n",
        "                        wandb.log({'ema_v_loss': ema_val_loss, 'ema_v_f1': ema_val_score})\n",
        "\n",
        "                        save_filename = filename.format(epoch=epoch, val_loss=ema_val_loss, val_score=ema_val_score) + '-ema.ckpt'\n",
        "                        torch.save({\"model\": ema_model.state_dict()}, save_filename)\n",
        "                        logger.info(f'{save_filename} : (ema) saved.')\n",
        "\n",
        "                    if best_model and filename:\n",
        "                        save_path = filename.format(epoch=epoch, val_loss=_val_loss, val_score=_val_score) + '.ckpt'\n",
        "                        checkpoint_filenames.append(save_path)\n",
        "                        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "                        torch.save({\"model\": best_model.state_dict()}, save_path)\n",
        "                        logger.info(f'{save_path} : saved.')\n",
        "                        if len(checkpoint_filenames) > 1:\n",
        "                            os.remove(checkpoint_filenames[-2])\n",
        "                    return best_model\n",
        "\n",
        "                if _val_score > best_score:\n",
        "                    best_score = _val_score\n",
        "                    best_loss = _val_loss\n",
        "                    best_model = model\n",
        "                    best_patience = MAX_PATIENCE\n",
        "                    if filename:\n",
        "                        save_path = filename.format(epoch=epoch, val_loss=_val_loss, val_score=_val_score) + '.ckpt'\n",
        "                        checkpoint_filenames.append(save_path)\n",
        "                        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "                        torch.save({\"model\": model.state_dict()}, save_path)\n",
        "                        logger.info(f'{save_path} : saved.')\n",
        "                        if len(checkpoint_filenames) > 1:\n",
        "                            os.remove(checkpoint_filenames[-2])\n",
        "                elif _val_loss < best_loss:\n",
        "                    best_loss = _val_loss\n",
        "                    best_patience = MAX_PATIENCE\n",
        "                else:\n",
        "                    best_patience -= 1\n",
        "\n",
        "    return best_model\n",
        "\n",
        "# Validation Function\n",
        "def validation(model, loss_fn, val_loader, device, use_amp):\n",
        "    model = model.to(device)\n",
        "    save_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    val_loss = []\n",
        "    preds, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "            true_labels += batch['label'].detach().cpu().numpy().tolist()\n",
        "            with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n",
        "                pred = model(batch['pixel_values'].to(device))\n",
        "                loss = loss_fn(pred, batch['label'].to(device))\n",
        "            preds += pred.detach().argmax(1).cpu().numpy().tolist()\n",
        "            val_loss.append(loss.item())\n",
        "\n",
        "        _val_loss = np.mean(val_loss)\n",
        "        _val_score = f1_score(true_labels, preds, average='macro')\n",
        "\n",
        "    if save_training:\n",
        "        model.train()\n",
        "    return _val_loss, _val_score\n",
        "\n",
        "# Prediction Function\n",
        "def prediction(model, test_loader, device):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Prediction\"):\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            pred = model(pixel_values)\n",
        "            preds += F.softmax(pred, dim=1).detach().cpu().numpy().tolist()\n",
        "    return preds\n",
        "\n",
        "# Initialize WandB\n",
        "def init_wandb(fold_idx, model_name, dt_str):\n",
        "    run = wandb.init(\n",
        "        name=f'fold{fold_idx+1}_{model_name}_' + dt_str,\n",
        "        config=CFG,\n",
        "        reinit=True,\n",
        "        mode='offline'\n",
        "    )\n",
        "    return run\n",
        "\n",
        "# Plot Training History\n",
        "def plot_training_history(history):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history['train_loss'], 'b', label='Training loss')\n",
        "    plt.plot(epochs, history['val_loss'], 'r', label='Validation loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot F1 Score\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history['val_f1'], 'g', label='Validation F1')\n",
        "    plt.title('Validation F1 Score')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Main Training Loop\n",
        "dt_str = datetime.now().strftime('%m%d%H%M')\n",
        "\n",
        "skf = StratifiedKFold(n_splits=CFG['N_SPLIT'], random_state=CFG['SEED'], shuffle=True)\n",
        "\n",
        "for fold_idx, (train_index, val_index) in enumerate(skf.split(train_df, train_df['class'])):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    logger.info(f'fold_idx={fold_idx} started')\n",
        "    run = init_wandb(fold_idx, CFG[\"MODEL_NAME\"].split(\"/\")[1].split(\"-\")[0], dt_str)\n",
        "\n",
        "    train_fold_df = train_df.loc[train_index]\n",
        "    val_fold_df = train_df.loc[val_index]\n",
        "\n",
        "    train_dataset = CustomDataset(\n",
        "        img_path_list=train_fold_df['img_path'].values,\n",
        "        label_list=train_fold_df['class'].values,\n",
        "        load_img_size=CFG['IMG_TRAIN_SIZE'],\n",
        "        shuffle=True,\n",
        "        transforms=train_transform\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=CFG['BATCH_SIZE'],\n",
        "        shuffle=True,\n",
        "        num_workers=8,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    val_dataset = CustomDataset(\n",
        "        img_path_list=val_fold_df['img_path'].values,\n",
        "        label_list=val_fold_df['class'].values,\n",
        "        load_img_size=CFG['IMG_SIZE'],\n",
        "        shuffle=False,\n",
        "        transforms=test_transform\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=CFG['BATCH_SIZE']*2,\n",
        "        shuffle=False,\n",
        "        num_workers=8,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    model = create_model(CFG['MODEL_NAME'])\n",
        "\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=CFG['LR'][0],\n",
        "        weight_decay=0.001,\n",
        "    )\n",
        "    scheduler = CosineAnnealingWarmupRestarts(\n",
        "        optimizer,\n",
        "        first_cycle_steps=int(len(train_loader)) // 4,\n",
        "        cycle_mult=1.0,\n",
        "        max_lr=CFG['LR'][0] * 2,\n",
        "        min_lr=CFG['LR'][1],\n",
        "        warmup_steps=0,\n",
        "        gamma=0.93,\n",
        "    )\n",
        "\n",
        "    model = train(\n",
        "        model, optimizer, train_loader, val_loader, scheduler,\n",
        "        device, validation_steps=10, logging_steps=10,\n",
        "        use_amp=(CFG['PRECISION'] == '16'),\n",
        "        filename=f'./ckpt/{CFG[\"MODEL_NAME\"].split(\"/\")[1].split(\"-\")[0]}-fold_idx={fold_idx}-epoch={{epoch:02d}}-val_loss={{val_loss:.4f}}-val_score={{val_score:.4f}}'\n",
        "    )\n",
        "\n",
        "    plot_training_history({\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_f1': []\n",
        "    })\n",
        "\n",
        "    model = None\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    logger.info(f'fold_idx={fold_idx} finished')\n",
        "    run.finish()\n",
        "\n",
        "    try:\n",
        "        last_chpt_info = os.popen(\"ls -t ./ckpt/ | head -n1\").read().strip()\n",
        "        last_chpt_info = ','.join(last_chpt_info[:-5].split('-')[1:])\n",
        "        os.system(f'python ~/send_telegram.py \"{last_chpt_info}\"')\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(os.listdir('./ckpt'))\n",
        "\n",
        "# Save Final Checkpoint\n",
        "torch.save({'model': model.state_dict()}, 'checkpoint_epoch_3.pth')"
      ],
      "metadata": {
        "id": "LVZFnLnBIPQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Training on Sample Dataset\n",
        "model = create_model(CFG['MODEL_NAME'])\n",
        "optimizer = optim.AdamW(model.parameters(), lr=CFG['LR'][0])\n",
        "scheduler = CosineAnnealingWarmupRestarts(\n",
        "    optimizer,\n",
        "    first_cycle_steps=int(len(train_loader_small)) // 4,\n",
        "    cycle_mult=1.0,\n",
        "    max_lr=CFG['LR'][0] * 2,\n",
        "    min_lr=CFG['LR'][1],\n",
        "    warmup_steps=0,\n",
        "    gamma=0.93\n",
        ")\n",
        "\n",
        "model = train(\n",
        "    model, optimizer, train_loader_small, val_loader, scheduler,\n",
        "    device, validation_steps=10, logging_steps=10,\n",
        "    use_amp=(CFG['PRECISION'] == '16'),\n",
        "    filename=f'./ckpt/{CFG[\"MODEL_NAME\"].split(\"/\")[1].split(\"-\")[0]}-fold_idx={{fold_idx}}-epoch={{epoch:02d}}-val_loss={{val_loss:.4f}}-val_score={{val_score:.4f}}'\n",
        ")"
      ],
      "metadata": {
        "id": "JCtkX8eTIQIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction and Ensemble\n",
        "test_df = pd.read_csv('./test.csv')\n",
        "\n",
        "ckpt_df = pd.DataFrame({'fname': glob('./ckpt/*.ckpt')})\n",
        "ckpt_df['mtime'] = ckpt_df.fname.apply(lambda x: int(os.stat(x).st_mtime))\n",
        "ckpt_df['model_name'] = ckpt_df.fname.apply(lambda x: re.search(r'./ckpt/(.*?)-fold', x).group(1) if re.search(r'./ckpt/(.*?)-fold', x) else None)\n",
        "ckpt_df['img_size'] = ckpt_df.fname.apply(lambda x: int(re.search(r'patch[0-9]+_([0-9]+)', x + 'patch0_0').group(1)) if re.search(r'patch[0-9]+_([0-9]+)', x + 'patch0_0') else 0)\n",
        "ckpt_df['is_ema'] = ckpt_df.fname.str.endswith('ema.ckpt').astype(int)\n",
        "ckpt_df['fold_idx'] = ckpt_df.fname.apply(lambda x: int(re.search(r'fold_idx=([0-9]+)-', x).group(1)) if re.search(r'fold_idx=([0-9]+)-', x) else -1)\n",
        "ckpt_df['val_loss'] = ckpt_df.fname.apply(lambda x: float(re.search(r'val_loss=(0\\.[0-9]+)', x).group(1)) if re.search(r'val_loss=(0\\.[0-9]+)', x) else None)\n",
        "ckpt_df['val_score'] = ckpt_df.fname.apply(lambda x: float(re.search(r'val_score=(0\\.[0-9]+)', x).group(1)) if re.search(r'val_score=(0\\.[0-9]+)', x) else None)\n",
        "\n",
        "ckpt_df = ckpt_df[(ckpt_df.img_size != 0) & (ckpt_df.is_ema == 0)]\n",
        "ckpt_df = ckpt_df.sort_values('mtime', ascending=False).reset_index(drop=True)\n",
        "ckpt_indexes = ckpt_df[ckpt_df.fold_idx == ckpt_df.fold_idx.max()].index[:4]\n",
        "\n",
        "preds = []\n",
        "preds_score = []\n",
        "\n",
        "for ckpt_start_index in ckpt_indexes:\n",
        "    logger.info(f'{ckpt_df.fname[ckpt_start_index]} loading')\n",
        "    CFG['IMG_SIZE'] = ckpt_df.img_size[ckpt_start_index]\n",
        "    assert CFG['IMG_SIZE'] in (196, 224)\n",
        "    logger.info(CFG['IMG_SIZE'])\n",
        "\n",
        "    test_dataset = CustomDataset(\n",
        "        test_df['img_path'].values, None,\n",
        "        interpolation=CFG['INTERPOLATION'], load_img_size=CFG['IMG_SIZE'],\n",
        "        shuffle=False, transforms=test_transform\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=CFG['BATCH_SIZE']*2,\n",
        "        shuffle=False,\n",
        "        num_workers=8,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    model_name = ckpt_df.model_name[ckpt_start_index]\n",
        "    model = create_model(model_name)\n",
        "    if ckpt_df.is_ema[ckpt_start_index]:\n",
        "        model = swa_utils.AveragedModel(model)\n",
        "\n",
        "    for i in range(ckpt_start_index, ckpt_start_index + skf.get_n_splits()):\n",
        "        checkpoint_path = ckpt_df.fname[i]\n",
        "        logger.info(f'{checkpoint_path} loading')\n",
        "        model.load_state_dict(torch.load(checkpoint_path)['model'])\n",
        "\n",
        "        preds_score.append(ckpt_df.val_score[i])\n",
        "        preds.append(prediction_with_tta(model, test_loader, device))\n",
        "\n",
        "preds = np.array(preds)\n",
        "preds_score = np.array(preds_score)\n",
        "\n",
        "# Weighted Averaging Ensemble\n",
        "weights = preds_score / preds_score.sum()\n",
        "ensemble_preds = np.tensordot(weights, preds, axes=([0], [0]))\n",
        "preds_labels = le.inverse_transform(ensemble_preds.argmax(-1))\n",
        "print(preds_labels)\n"
      ],
      "metadata": {
        "id": "oXrGFZLlISaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Submission\n",
        "submit = pd.read_csv('./sample_submission.csv')\n",
        "submit['label'] = preds_labels\n",
        "dt_str = datetime.now().strftime('%Y%m%d_%H%M')\n",
        "submit.to_csv(f'./basslibrary_submit_{dt_str}.csv', index=False)\n",
        "logger.info(f'./basslibrary_submit_{dt_str}.csv saved')\n",
        "\n",
        "submit.label.value_counts()\n",
        "\n",
        "# Send Telegram Notification\n",
        "try:\n",
        "    os.system(f'python ~/send_telegram.py \"basslibrary_submit_{dt_str}.csv saved\"')\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "ga-GNdNSIUKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UdMIuj6lIWeB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}